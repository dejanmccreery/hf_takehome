{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6a9eae",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9568d30",
   "metadata": {},
   "source": [
    "### Problem Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb40d2",
   "metadata": {},
   "source": [
    "Problem set is 299 weekly time series that we need to filter through to predict the values for two target series.\n",
    "I have decided to break this into two univariate analyses based on the idea that we should ascertain the significance of our features to each of the two target series. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f1fc4",
   "metadata": {},
   "source": [
    "### Data and modelling approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b30710",
   "metadata": {},
   "source": [
    "**The data:**\n",
    "\n",
    "Firstly, we investigate the data by looking at summary statistics for the dataframe. We then split the data into a matrix of features (X) and a dataframe containing the two target variable vectors (y), before visualising the time series in appropriately sized tranches. As there are 299 features we visualise in tranches of 50.  \n",
    "\n",
    "This presents numerous outliers. Further inspection shows certain series have means away from 0. \n",
    "Given the context that this project is for a hedge fund, and most of the series have means around 0, we might be able to assume that these can be taken as weekly returns data.\n",
    "We will instead take a naive approach to the dataset.\n",
    "\n",
    "There are multiple series with missing values. These series do not seem to be unclean, but rather it seems the series start at different points in time. There do not seem to be missing values between the start and end of non-null values in any given series.\n",
    "\n",
    "Certain series exhibit seasonality, while others do not. Plotting on a monthly, quarterly and yearly basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574dc7e",
   "metadata": {},
   "source": [
    "**The Model:**\n",
    "\n",
    "In this analysis we opt to use XGBoost. This is for multiple reasons: \n",
    "\n",
    "    1 - XGBoost is insensitive to differently scaled features so long as we use a decision tree model as its basis.\n",
    "        The same goes for multicollinearity.\n",
    "        \n",
    "    2 - XGBoost provides in-built cross-validation.\n",
    "    \n",
    "    3 - XGBoost is less affected by NaN values as it bins the datapoints that are non-null and ignores series that have so few values that they are useless.\n",
    "    \n",
    "    4 - XGBoost is able to recognise seasonal and trend patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9875fb",
   "metadata": {},
   "source": [
    "**Cleaning:**\n",
    "\n",
    "As there are no missing values we therefore assume that there does not need to be any imputation on the raw data. \n",
    "\n",
    "We remove rows with nan values in the target series, before splitting the dataset into the feature matrix and a dataframe containing the two target variable vectors.\n",
    "\n",
    "We remove columns with less non-null values than the 20% required for a full test set in a classic 80.20 train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2a448",
   "metadata": {},
   "source": [
    "**Feature Selection:**\n",
    "\n",
    "We opt for pearson correlation comparison as an easy way to distinguish between useful and unuseful features. This is due to the simplicity, and the fact that XGBoost will be able to handle series of differing lengths.\n",
    "\n",
    "Relevant features are selected based on a criteria of being either above 0.3 or below -0.3. This is because pearson coefficients below these thresholds are negligible. We keep low scoring features (between 0.3 and 0.5, and between -0.3 and -0.5) so as to maximise information. It turns out XGBoost effectively deals with these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb73cd8",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8703a9c",
   "metadata": {},
   "source": [
    "Initial regression yielded satisfactory performance. The R-squared scores and RMSE values were as follows:\n",
    "\n",
    "Target 1:\n",
    "R^2 - 0.984323; \n",
    "RMSE - 0.00859\n",
    "\n",
    "Target 2:\n",
    "R^2 - 0.949335; \n",
    "RMSE - 0.03925\n",
    "\n",
    "However, multiple features in each feature set had an importance score of zero. This meant they were unused, and should be jettisoned.\n",
    "\n",
    "It was also possible that shorter series used in the model were unnecessary to its performance, so they were also considered for removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8854e0",
   "metadata": {},
   "source": [
    "**Tuning:**\n",
    "\n",
    "After training and testing the model on these first sets of selected features we removed the features with lower counts of non-null values. We settled on an arbitrary bound of 510 to cut out shorter series, but included series with less than 10 fewer non-null values. This was done to maximise the information passed to the model, essentially just in case series with 508 or 514 values were important to performance.\n",
    "\n",
    "The model was then refined by removing the scores with importance scores of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45336de5",
   "metadata": {},
   "source": [
    "**Results of Tuning:**\n",
    "\n",
    "Removing the features with importance scores of 0 left R^2 unchanged. The RMSE was of course slightly different due to the randomness of the tree model.\n",
    "\n",
    "Removing the shorter series yielded the following results:\n",
    "\n",
    "Target 1: R^2 - 0.984428; RMSE - 0.00859\n",
    "\n",
    "Target 2: R^2 - 0.949335; RMSE - 0.03923\n",
    "\n",
    "The R^2 scores were essentially unchanged. As R^2 is vulnerable throwing as many features as possible at the model, the fact it is unchanged by using less features is gratifying.\n",
    "\n",
    "RMSE is again variable, but it is included for completeness. It was also about the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb037a5",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365cb50a",
   "metadata": {},
   "source": [
    "Solely based on correlating individual features with the two target sets, we see the following:\n",
    "\n",
    "Most Significant to Target 1:\n",
    "- Feature 251\n",
    "- Feature 267\n",
    "- Feature 138\n",
    "\n",
    "Significant to Target 1:\n",
    "- Feature 266\n",
    "- Feature 297\n",
    "\n",
    "Most Significant to Target 2:\n",
    "- Feature 260\n",
    "- Feature 261\n",
    "- Feature 264\n",
    "\n",
    "Significant to Target 2:\n",
    "- Feature 265\n",
    "- Feature 299\n",
    "- Feature 297\n",
    "- Feature 298\n",
    "- Feature 261\n",
    "- Feature 262\n",
    "- Feature 294\n",
    "- Feature 175"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097a70f",
   "metadata": {},
   "source": [
    "After tuning XGBoost, its importance score shows us the most significant factors are as follows:\n",
    "\n",
    "Target 1:\n",
    "- Feature 251 by far (0.94)\n",
    "- Feature 267 (0.02)\n",
    "- Feature 266 (0.007)\n",
    "\n",
    "Target 2:\n",
    "- Feature 260 by far (0.92)\n",
    "- Feature 267 (0.022)\n",
    "- Feature 255 (0.014)\n",
    "- Feature 254 (0.009)\n",
    "- Feature 257 (0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b4cfb",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edb943",
   "metadata": {},
   "source": [
    "- Making the code more pythonic by defining classes/functions to avoid reuse of code\n",
    "- Improving individual feature selection by running simple OLS regressions instead of simply finding the Pearson coefficient\n",
    "- Further tuning potentially by reducing features further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
